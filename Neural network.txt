import numpy as np
import pandas as pd
from pathlib import Path
import yaml
import json
import joblib # for save_meta

import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn

from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import StandardScaler

from tqdm import tqdm

# --- src/data_generation.py ---
def generate_multivariate_ts(n_steps=2000, n_features=5, seed=42):
    rng = np.random.default_rng(seed)
    t = np.arange(n_steps)

    # Create shared trend
    trend = 0.0005 * t

    # Seasonality (weekly + yearly-smoothed)
    weekly = 2.0 * np.sin(2 * np.pi * t / 7)
    yearly = 1.5 * np.sin(2 * np.pi * t / 365.25)

    data = {}
    for i in range(n_features):
        scale = 0.5 + 0.5 * i / (n_features - 1)
        noise = rng.normal(0, 0.5 * scale, size=n_steps)
        # each feature mixes trend, seasonality with slightly different phase/amplitude
        phase = (i / n_features) * np.pi
        feature = 10 * (1 + trend) + scale * (weekly * np.cos(phase) + yearly * np.sin(phase)) + noise
        data[f"feat_{i}"] = feature

    # add an external regressor (e.g., holiday indicator or exog continuous)
    exog = 0.3 * np.sin(2 * np.pi * t / 30) + 0.1 * rng.normal(size=n_steps)
    data["exog_1"] = exog

    # target will be a combination of features
    df = pd.DataFrame(data)
    df["target"] = 0.6 * df["feat_0"] + 0.3 * df["feat_1"] + 0.1 * df["exog_1"] + rng.normal(0, 0.3, n_steps)
    df["ds"] = pd.date_range(start="2018-01-01", periods=n_steps, freq="D")
    cols = ["ds"] + [c for c in df.columns if c != "ds"]
    return df[cols]

# --- src/preprocess.py ---
def load_data(path="data/synthetic_multivar.csv"):
    df = pd.read_csv(path, parse_dates=["ds"])
    return df

def split_df(df, train_frac=0.7, val_frac=0.15):
    n = len(df)
    n_train = int(n * train_frac)
    n_val = int(n * val_frac)
    train = df.iloc[:n_train].reset_index(drop=True)
    val = df.iloc[n_train:n_train+n_val].reset_index(drop=True)
    test = df.iloc[n_train+n_val:].reset_index(drop=True)
    return train, val, test

def scale_features(train, val, test, feature_cols):
    scaler = StandardScaler()
    scaler.fit(train[feature_cols])
    train_s = train.copy()
    val_s = val.copy()
    test_s = test.copy()
    for d in (train_s, val_s, test_s):
        d[feature_cols] = scaler.transform(d[feature_cols])
    return train_s, val_s, test_s, scaler

def create_input_output(df, input_window=30, horizon=7, feature_cols=None):
    X, y = [], []
    arr = df[feature_cols].values
    for i in range(len(arr) - input_window - horizon + 1):
        X.append(arr[i:i+input_window])
        y.append(arr[i+input_window:i+input_window+horizon, feature_cols.index("target")] if "target" in feature_cols else arr[i+i+input_window:i+input_window+horizon, 0])
    X = np.stack(X)
    y = np.stack(y)
    return X, y

def save_meta(scaler, path="outputs/scaler.pkl"):
    Path("outputs").mkdir(exist_ok=True)
    joblib.dump(scaler, path)

# --- src/dataset.py ---
class TimeSeriesDataset(Dataset):
    def _init_(self, X, y):
        # X: (N, input_window, n_features), y: (N, horizon)
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def _len_(self):
        return len(self.X)

    def _getitem_(self, idx):
        return self.X[idx], self.y[idx]

# --- src/model.py (placeholder, as it was missing) ---
class SeqModel(nn.Module):
    def _init_(self, input_dim, hidden_size, num_layers, dropout, rnn_type, output_horizon):
        super()._init_()
        self.rnn_type = rnn_type
        if rnn_type == "LSTM":
            self.rnn = nn.LSTM(input_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)
        elif rnn_type == "GRU":
            self.rnn = nn.GRU(input_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)
        else: # Default to RNN if type is not recognized
            self.rnn = nn.RNN(input_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_horizon)

    def forward(self, x):
        # x: (batch_size, input_window, input_dim)
        output, _ = self.rnn(x)
        # We take the output of the last time step for prediction
        # output: (batch_size, input_window, hidden_size)
        return self.fc(output[:, -1, :])

# --- src/train.py ---
def rmse(a,b): return np.sqrt(mean_squared_error(a,b))

def train_loop(cfg):
    # Load data (generate if missing)
    Path("data").mkdir(exist_ok=True)
    data_file_path = "data/synthetic_multivar.csv"
    if not Path(data_file_path).exists():
        print("Generating synthetic data...")
        df_generated = generate_multivariate_ts(n_steps=cfg["data"]["n_steps"], n_features=cfg["data"]["n_features"])
        df_generated.to_csv(data_file_path, index=False)
        df = df_generated # Use the generated df
    else:
        df = load_data(data_file_path) # Load existing df

    train, val, test = split_df(df, cfg["data"]["train_frac"], cfg["data"]["val_frac"])
    feature_cols = [c for c in df.columns if c not in ("ds",)]
    train_s, val_s, test_s, scaler = scale_features(train, val, test, feature_cols)
    save_meta(scaler)

    X_train, y_train = create_input_output(train_s, cfg["data"]["input_window"], cfg["data"]["forecast_horizon"], feature_cols)
    X_val, y_val = create_input_output(val_s, cfg["data"]["input_window"], cfg["data"]["forecast_horizon"], feature_cols)

    train_ds = TimeSeriesDataset(X_train, y_train)
    val_ds = TimeSeriesDataset(X_val, y_val)

    train_loader = DataLoader(train_ds, batch_size=cfg["training"]["batch_size"], shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=cfg["training"]["batch_size"], shuffle=False)

    device = torch.device(cfg["training"]["device"] if torch.cuda.is_available() else "cpu")
    model = SeqModel(input_dim=len(feature_cols), hidden_size=cfg["model"]["hidden_size"],
                     num_layers=cfg["model"]["num_layers"], dropout=cfg["model"]["dropout"],
                     rnn_type=cfg["model"]["type"], output_horizon=cfg["data"]["forecast_horizon"]).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg["training"]["lr"], weight_decay=cfg["training"]["weight_decay"])
    loss_fn = torch.nn.MSELoss()

    best_val = float("inf")
    Path("checkpoints").mkdir(exist_ok=True)
    metrics = {"train_loss": [], "val_loss": []}

    for epoch in range(1, cfg["training"]["epochs"] + 1):
        model.train()
        train_losses = []
        for Xb, yb in train_loader:
            Xb, yb = Xb.to(device), yb.to(device)
            pred = model(Xb)
            loss = loss_fn(pred, yb)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())
        avg_train = float(np.mean(train_losses))

        # val
        model.eval()
        val_losses = []
        preds, trues = [], []
        with torch.no_grad():
            for Xb, yb in val_loader:
                Xb, yb = Xb.to(device), yb.to(device)
                p = model(Xb)
                val_losses.append(loss_fn(p, yb).item())
                preds.append(p.cpu().numpy())
                trues.append(yb.cpu().numpy())
        avg_val = float(np.mean(val_losses))
        metrics["train_loss"].append(avg_train)
        metrics["val_loss"].append(avg_val)
        print(f"Epoch {epoch} | train_loss {avg_train:.4f} | val_loss {avg_val:.4f}")

        if avg_val < best_val:
            best_val = avg_val
            torch.save(model.state_dict(), "checkpoints/best_model.pt")
            print("Saved best model.")

    # Save metrics
    Path("outputs").mkdir(exist_ok=True)
    with open("outputs/metrics.json", "w") as f:
        json.dump(metrics, f)
    print("Training complete.")

if _name_ == "_main_":
    cfg = {}
    config_path = Path("config.yaml")
    if not config_path.exists():
        print("config.yaml not found. Creating a default config for training.")
        default_cfg = {
            "data": {
                "n_steps": 2000,
                "n_features": 5,
                "train_frac": 0.7,
                "val_frac": 0.15,
                "input_window": 30,
                "forecast_horizon": 7
            },
            "model": {
                "hidden_size": 64,
                "num_layers": 2,
                "dropout": 0.2,
                "type": "LSTM"
            },
            "training": {
                "epochs": 5,
                "batch_size": 32,
                "lr": 0.001,
                "weight_decay": 1e-5,
                "device": "cuda"
            }
        }
        with open(config_path, "w") as f:
            yaml.safe_dump(default_cfg, f)
        cfg = default_cfg
    else:
        with open(config_path) as f:
            cfg = yaml.safe_load(f)

    train_loop(cfg)
    # src/evaluate.py
# import yaml # already imported
# import numpy as np # already imported
# import torch # already imported
# from pathlib import Path # already imported
# import json # already imported
import matplotlib.pyplot as plt


def rmse(a,b): return np.sqrt(mean_squared_error(a,b))

def evaluate(cfg):
    df = load_data("data/synthetic_multivar.csv")
    train, val, test = split_df(df, cfg["data"]["train_frac"], cfg["data"]["val_frac"])
    feature_cols = [c for c in df.columns if c not in ("ds",)]
    _, _, test_s, _ = scale_features(train, val, test, feature_cols)

    X_test, y_test = create_input_output(test_s, cfg["data"]["input_window"], cfg["data"]["forecast_horizon"], feature_cols)
    test_ds = TimeSeriesDataset(X_test, y_test)
    loader = DataLoader(test_ds, batch_size=cfg["training"]["batch_size"], shuffle=False)
    device = torch.device(cfg["training"]["device"] if torch.cuda.is_available() else "cpu")
    model = SeqModel(input_dim=len(feature_cols), hidden_size=cfg["model"]["hidden_size"],
                     num_layers=cfg["model"]["num_layers"], dropout=cfg["model"]["dropout"],
                     rnn_type=cfg["model"]["type"], output_horizon=cfg["data"]["forecast_horizon"]).to(device)
    model.load_state_dict(torch.load("checkpoints/best_model.pt", map_location=device))
    model.eval()

    preds, trues = [], []
    with torch.no_grad():
        for Xb, yb in loader:
            Xb = Xb.to(device)
            p = model(Xb).cpu().numpy()
            preds.append(p)
            trues.append(yb.numpy())
    preds = np.vstack(preds)
    trues = np.vstack(trues)

    metrics = {
        "rmse": float(rmse(trues.reshape(-1), preds.reshape(-1))),
        "mae": float(mean_absolute_error(trues.reshape(-1), preds.reshape(-1)))
    }
    Path("outputs").mkdir(exist_ok=True)
    with open("outputs/eval_metrics.json", "w") as f:
        json.dump(metrics, f, indent=2)
    print("Evaluation metrics:", metrics)

    # plot first 100 true vs pred for first horizon step
    plt.figure(figsize=(10,4))
    plt.plot(trues[:200,0], label="true")
    plt.plot(preds[:200,0], label="pred")
    plt.legend()
    plt.title("First forecast step: true vs pred (test)")
    plt.savefig("outputs/figures_true_vs_pred.png")
    print("Saved plot to outputs/figures_true_vs_pred.png")

if _name_ == "_main_":
    Path("outputs/figures").mkdir(parents=True, exist_ok=True)
    cfg = {}
    config_path = Path("config.yaml")
    if not config_path.exists():
        print("config.yaml not found. Creating a default config for evaluation.")
        default_cfg = {
            "data": {
                "n_steps": 2000,
                "n_features": 5,
                "train_frac": 0.7,
                "val_frac": 0.15,
                "input_window": 30,
                "forecast_horizon": 7
            },
            "model": {
                "hidden_size": 64,
                "num_layers": 2,
                "dropout": 0.2,
                "type": "LSTM"
            },
            "training": {
                "epochs": 5,
                "batch_size": 32,
                "lr": 0.001,
                "weight_decay": 1e-5,
                "device": "cuda"
            }
        }
        with open(config_path, "w") as f:
            yaml.safe_dump(default_cfg, f)
        cfg = default_cfg
    else:
        with open(config_path) as f:
            cfg = yaml.safe_load(f)
    evaluate(cfg)
    # src/baseline.py
import warnings
warnings.filterwarnings("ignore")
# import yaml # already imported
import pandas as pd
# from pathlib import Path # already imported
from statsmodels.tsa.statespace.sarimax import SARIMAX
# from sklearn.metrics import mean_squared_error, mean_absolute_error # already imported
# import numpy as np # already imported
# import json # already imported

def baseline_sarimax(cfg):
    df = pd.read_csv("data/synthetic_multivar.csv", parse_dates=["ds"])
    train_frac = cfg["data"]["train_frac"]
    n_train = int(len(df) * train_frac)
    train = df.iloc[:n_train]
    test = df.iloc[n_train:]

    endog = train["target"]
    exog = train[[c for c in df.columns if c.startswith("exog")]] if any(c.startswith("exog") for c in df.columns) else None

    order = tuple(cfg["baseline"]["sarimax_order"])
    seasonal = tuple(cfg["baseline"]["seasonal_order"])
    model = SARIMAX(endog, exog=exog, order=order, seasonal_order=seasonal, enforce_stationarity=False, enforce_invertibility=False)
    res = model.fit(disp=False)
    # produce dynamic forecasts for the test set
    exog_test = test[[c for c in df.columns if c.startswith("exog")]] if exog is not None else None
    preds = res.get_forecast(steps=len(test), exog=exog_test).predicted_mean.values
    metrics = {
        "rmse": float(np.sqrt(mean_squared_error(test["target"].values, preds))),
        "mae": float(mean_absolute_error(test["target"].values, preds))
    }
    Path("outputs").mkdir(exist_ok=True)
    with open("outputs/baseline_metrics.json","w") as f:
        json.dump(metrics, f, indent=2)
    print("Baseline metrics:", metrics)

if _name_ == "_main_":
    config_path = Path("config.yaml")
    if not config_path.exists():
        print("config.yaml not found. Creating a default config for baseline.")
        default_cfg = {
            "data": {
                "n_steps": 2000,
                "n_features": 5,
                "train_frac": 0.7,
                "val_frac": 0.15,
                "input_window": 30,
                "forecast_horizon": 7
            },
            "model": {
                "hidden_size": 64,
                "num_layers": 2,
                "dropout": 0.2,
                "type": "LSTM"
            },
            "training": {
                "epochs": 5,
                "batch_size": 32,
                "lr": 0.001,
                "weight_decay": 1e-5,
                "device": "cuda"
            },
            "baseline": {
                "sarimax_order": [1, 1, 1],
                "seasonal_order": [1, 1, 0, 7]
            }
        }
        with open(config_path, "w") as f:
            yaml.safe_dump(default_cfg, f)
        cfg = default_cfg
    else:
        with open(config_path) as f:
            cfg = yaml.safe_load(f)
        # Ensure baseline config exists if it wasn't in the loaded config
        if "baseline" not in cfg:
            cfg["baseline"] = {"sarimax_order": [1, 1, 1], "seasonal_order": [1, 1, 0, 7]}
            print("Added default baseline config to existing config.yaml.")
            with open(config_path, "w") as f:
                yaml.safe_dump(cfg, f) # Save updated config

    baseline_sarimax(cfg)
    # src/explain.py
import shap
import yaml
import numpy as np
import torch
from pathlib import Path
# from preprocess import load_data, split_df, scale_features, create_input_output
# from model import SeqModel
# from dataset import TimeSeriesDataset
# from torch.utils.data import DataLoader

def explain(cfg, nsamples=100):
    df = load_data("data/synthetic_multivar.csv")
    train, val, test = split_df(df, cfg["data"]["train_frac"], cfg["data"]["val_frac"])
    feature_cols = [c for c in df.columns if c not in ("ds",)]
    train_s, val_s, test_s, scaler = scale_features(train, val, test, feature_cols)

    # Create smaller sample to explain
    X_val, y_val = create_input_output(val_s, cfg["data"]["input_window"], cfg["data"]["forecast_horizon"], feature_cols)
    X_sample = X_val[:nsamples]  # shape (N, window, features)
    # convert model for shap
    device = torch.device(cfg["training"]["device"] if torch.cuda.is_available() else "cpu")
    model = SeqModel(input_dim=len(feature_cols), hidden_size=cfg["model"]["hidden_size"],
                     num_layers=cfg["model"]["num_layers"], dropout=cfg["model"]["dropout"],
                     rnn_type=cfg["model"]["type"], output_horizon=cfg["data"]["forecast_horizon"]).to(device)
    model.load_state_dict(torch.load("checkpoints/best_model.pt", map_location=device))
    model.eval()

    # Wrap model: SHAP expects a function mapping 2D arrays -> outputs. We'll flatten the time dimension.
    def model_predict(X_flat):
        # X_flat shape: (m, window*features)
        X = X_flat.reshape((-1, cfg["data"]["input_window"], len(feature_cols))).astype(np.float32)
        with torch.no_grad():
            X_t = torch.tensor(X).to(device)
            out = model(X_t).cpu().numpy()
        return out

    # create background by sampling from training set
    X_train, _ = create_input_output(train_s, cfg["data"]["input_window"], cfg["data"]["forecast_horizon"], feature_cols)
    background = X_train[np.random.choice(len(X_train), size=min(100, len(X_train)), replace=False)]
    explainer = shap.KernelExplainer(model_predict, background.reshape((background.shape[0], -1)))
    shap_values = explainer.shap_values(X_sample.reshape((X_sample.shape[0], -1)), nsamples=50)
    # shap_values for each forecast horizon: list length = horizon, each array shape (N, window*features)
    Path("outputs").mkdir(exist_ok=True)
    # Save shap values (large) as numpy
    np.save("outputs/shap_values.npy", shap_values)
    print("Saved SHAP explanations to outputs/shap_values.npy")

if _name_ == "_main_":
    Path("outputs").mkdir(exist_ok=True)
    with open("config.yaml") as f:
        cfg = yaml.safe_load(f)
    explain(cfg)